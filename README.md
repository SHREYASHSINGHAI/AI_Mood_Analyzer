# ğŸ§  AI Mood Analyzer

An end-to-end AI Mood Analyzer that detects emotions from text using a trained machine learning model.
The project is designed with production-grade ML architecture, separating training, inference, and user interface layers.


ğŸ› ï¸ Tech Stack

- Python 3.11
- Scikit-learn
- FastAPI
- Pydantic
- Streamlit
- MLflow
- Docker


ğŸš€ Project Highlights

ğŸ” Text-based emotion detection
ğŸ§  ML model trained using Scikit-learn
âš¡ FastAPI-based inference service
ğŸ¨ Streamlit frontend for user interaction
ğŸ“¦ Docker-ready architecture


ğŸ§  Model Details

Algorithm: Logistic Regression (One-vs-Rest)
Vectorization: TF-IDF
Frameworks:
Scikit-learn
MLflow (experiment tracking ready)

The model is trained offline and saved as artifacts, which are later loaded by the inference API.

ğŸ”Œ Inference API (FastAPI)

Endpoint: POST /predict
Request Body:
{
  "texts": ["I feel excited about my new job!"]
}
Response:
{
  "emotion": "happy"
}


Features

Input validation using Pydantic
Batch prediction support
Clean error handling
Health-check ready architecture


ğŸ¨ Frontend (Streamlit)

Simple and intuitive UI
Sends user input to FastAPI
Displays detected emotion
Handles API failures gracefully
The Streamlit app does NOT load the model directly â€” it communicates only via the API


ğŸ§± System Architecture

User (Streamlit UI)
        â†“
FastAPI Inference Service
        â†“
Saved ML Artifacts (model, vectorizer)
        â†‘
Offline Training Pipeline


------------------------------------------------------------------------------------------
âš™ï¸ How to Run the Project Locally

1. Create & activate virtual environment:
     python -m venv venv
     venv\Scripts\activate   # Windows

2. Install backend dependencies:
     pip install -r requirements.txt
   
3. Run Inference API:
     uvicorn inference.app:app --host 0.0.0.0 --port 8000 --reload

4. Check API on the browser:
      http://localhost:8000/docs

5. Run Streamlit App:
     cd streamlit_app
     pip install -r requirements.txt
     streamlit run app.py

6. Open in browser:
     http://localhost:8501

---------------------------------------------------------------------------------------

ğŸ³ Docker Support

Dockerfile.train â†’ Training pipeline
Dockerfile.infer â†’ Inference service

The project is container-ready and can be extended with Docker Compose.


ğŸ“Œ Configuration Management

config.json stores model-related metadata
Keeps parameters version-controlled
Helps ensure reproducibility and clarity


ğŸ¯ Why This Project Matters

This project demonstrates:

âœ… End-to-end ML system design
âœ… Real-world API + UI integration
âœ… Clean code organization
âœ… Debugging & production thinking
âœ… MLOps fundamentals (without overengineering)

Author: Shreyash Singhai
